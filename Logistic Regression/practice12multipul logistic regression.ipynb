{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (x,betas) :\n",
    "    m = x.shape[0]\n",
    "    x_bais = np.ones((m,1))\n",
    "    x = np.c_[x,x_bais]\n",
    "    z = x.dot(betas)\n",
    "\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "def cost_func (x,y,betas) : \n",
    "    y_hat = sigmoid(x, betas)\n",
    "    m = x.shape[0]\n",
    "    loss = -1/m * sum(y.T.dot(  np.log(y_hat)) + (1-y).T.dot( np.log (1-y_hat)))\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(alpha, iterations, x, y):\n",
    "    m = x.shape[0]\n",
    "    x_bias = np.ones((m,1))\n",
    "    x = np.c_[x_bias, x]\n",
    "    k = x.shape[1]\n",
    "    betas = np.random.randn(k,1)\n",
    "    for i in range(iterations):\n",
    "        y_hat = sigmoid(x, betas)\n",
    "        betas = betas - alpha/ m * (x.T.dot(y_hat - y))\n",
    "    return betas\n",
    "\n",
    "def predict (x, betas):\n",
    "    return sigmoid(x, betas)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
